<!DOCTYPE html>

<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="UniG3D: A Unified 3D Object Generation Dataset.">
    <meta name="author" content="Qinghong Sun">

    <title>UniG3D: A Unified 3D Object Generation Dataset</title>

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
        <link rel="icon" href="imgs/logo.png" type="image/x-ico">



</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <!-- <img src="img/oo3d_teaser.gif" style="width:30%; margin-right:0px; margin-top:0px;"> -->
    <h1 class="nerf_title_v2">UniG3D</h1>
    <h1 class="nerf_subheader_v2">A Unified 3D Object Generation Dataset</h1>
<!--     <h1 class="nerf_subheader_v2">CVPR 2023 (Award Candidate)</h1> -->
    <hr>
    <p class="authors">
      Authors
<!--       <a href=https://wutong16.github.io/ target="_blank"> Tong Wu</a><sup>1,2</sup>,
      <a href=> Jiarui Zhang</a><sup>1,3</sup>,
      <a href=https://fuxiao0719.github.io/ target="_blank"> Xiao Fu</a><sup>1</sup>,
      <a href=> Yuxin Wang</a><sup>1,4</sup>,
      <a href=https://jiawei-ren.github.io/ target="_blank"> Jiawei Ren</a><sup>5</sup>,
      <a href=https://scholar.google.com/citations?user=lSDISOcAAAAJ&hl=zh-CN target="_blank"> Liang Pan</a><sup>5</sup>,
        <br>
      <a href=https://wywu.github.io/ target="_blank"> Wayne Wu</a><sup>1</sup>,
      <a href=https://scholar.google.com.hk/citations?user=jZH2IPYAAAAJ&hl=en target="_blank"> Lei Yang</a><sup>1,3</sup>,
      <a href=https://myownskyw7.github.io/ target="_blank"> Jiaqi Wang</a><sup>1</sup>,
      <a href=https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&hl=zh-CN&user=AerkT0YAAAAJ&sortby=pubdate target="_blank"> Chen Qian</a><sup>1</sup>,
      <a href=https://scholar.google.com/citations?user=GMzzRRUAAAAJ&hl=zh-CN target="_blank"> Dahua Lin</a><sup>1,2</sup>&#9993,
      <a href=https://liuziwei7.github.io/ target="_blank"> Ziwei Liu</a><sup>5</sup>&#9993 -->
    </p>

<!--     <h1 class="nerf_subheader_v2">
      <sup>1</sup>Shanghai Artificial Intelligence Laboratory, 
      <sup>2</sup>The Chinese University of Hong Kong,
      <sup>3</sup>SenseTime Research, <br>
      <sup>4</sup>Hong Kong University of Science and Technology,
      <sup>5</sup>S-Lab, Nanyang Technological University
      <!-- <sup>6</sup>Centre of Perceptual and Interactive Intelligence -->
<!--     </h1> -->
    <!-- <div class="nerf_equal_v2"><span class="text-span_nerf">*</span><span class="text-span_nerf_star">+</span>Corresponding Author</div> -->

    <!-- </br></br> -->
<!--     <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2301.07525" target="_blank">Paper</a>
        <a class="btn btn-primary" href="https://github.com/omniobject3d/OmniObject3D/tree/main" target="_blank">Code</a>
        <a class="btn btn-primary" href="https://opendatalab.com/OpenXD-OmniObject3D-New/download" target="_blank">Dataset</a>
    </div> -->
</div>

    
<div class="container">

<!--<hr>-->
    </br></br>
    <div data-anchor="slide1" class="section nerf_section">
        <div class="grey_container w-container">
            <h2 class="grey-heading_nerf">
                <b>Abstract</b>
            </h2>
            <p class="paragraph-3 nerf_text">
                We propose UniG3D, a unified 3D object generation dataset constructed by employing a universal data transformation pipeline on Objaverse and ShapeNet datasets. This pipeline converts each raw 3D model into comprehensive multi-modal data representation <text, image, point cloud, mesh> by employing rendering engines and multi-modal models. These modules ensure the richness of textual information and the comprehensiveness of data representation.
            </p>   
            <img src="imgs/fig2.png" style="width:75%; margin-right:0px; margin-top:0px;">
            <p class="paragraph-3 nerf_text">
                <br>
                UniG3D offers three contributions: 
                <br>
                <b>1) Comprehensive Multi-Modal Data:</b> We construct a large-scale unified 3D object generation dataset with rich textural information.
                <br>
                <b>2) Universal Data Transformation Pipeline:</b> We propose a universal data transformation pipeline that can convert any 3D data into representations suitable for most 3D object generation methods.
                <br>
                <b>3) Valuable insights:</b> To validate the efficacy of our dataset, we conduct experiments under various input conditions and target 3D representations. Based on our empirical investigations, we present several valuable insights into the impact of various conditions, the efficacy of data expansion, and the significance of text quality.
                <br>
            </p>
        </div>
    </div>

</br></br>
<div class="section">
    <s2> Statistics </s2>
            <!-- <h2 class="grey-heading_nerf">
                <b>OmniObject3D Dataset</b>
            </h2> -->
            <hr>
            <!-- <p class="paragraph-3 nerf_text">
                
                OO3D is good
            </p> -->
            
            <div class="columns-5 w-row">
                <style type="text/css">
                    .tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}
                    .tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
                      font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:3px 3px;word-break:normal;}
                    .tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
                      font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
                    .tg .tg-baqh{text-align:center;vertical-align:top}
                    .tg .tg-6qw1{background-color:#c0c0c0;text-align:center;vertical-align:top}
                    .tg .tg-6qw2{background-color:#e4e1e1;text-align:center;vertical-align:top}
                    </style>
                    <table class="tg" style="undefined;table-layout: fixed; width: 582px; margin-left:auto;margin-right:auto;">
                    <colgroup>
                    <col style="width: 90px">
                    <col style="width: 70px">
                    <col style="width: 70px">
                    <col style="width: 70px">
                    <col style="width: 70px">
                    </colgroup>
                    <thead>
                      <tr>
                        <th class="tg-6qw1">Dataset</th>
                        <th class="tg-6qw1">#Mesh</th>
                        <th class="tg-6qw1">#PCL</th>
                        <th class="tg-6qw1">#Image</th>
                        <th class="tg-6qw1">#Text</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td class="tg-baqh">UniG3D-<a href=https://shapenet.org/ target="_blank">ShapeNet</a></td>
                        <td class="tg-baqh">500K</td>
                        <td class="tg-baqh">50K</td>
                        <td class="tg-baqh">1 million</td>
                        <td class="tg-baqh">50K</td>
                      </tr>
                      <tr>
                        <td class="tg-baqh">UniG3D-<a href=https://objaverse.allenai.org/ target="_blank">Objaverse</a></td>
                        <td class="tg-baqh">5 million</td>
                        <td class="tg-baqh">500K</td>
                        <td class="tg-baqh">10 million</td>
                        <td class="tg-baqh">500K</td>
                      </tr>
                    </tbody>
                    </table>
            </div>
          
          </br>
            
          <p class="paragraph-3 nerf_text", style="text-align:center">
              Table 1. The statistical information of the four representations in two UniG3D datasets.
          </p>
          
        </br>
            
          <div class="columns-5 w-row">
                <img src="imgs/dataset.png" style="width:50%; margin-right:0px; margin-top:10px;margin-bottom:0px;">
          </div>

        </br>

          <p class="paragraph-3 nerf_text", style="text-align:center">
            Figure 1. A histogram of fine-grained UniG3D categories with representative members from several bins highlighted..
          </p>
    </div>

    <div class="section">
        <s2>Showcase</s2>
        <hr>
        
        <div class="columns-5 w-row">
                <img src="imgs/word_cloud.png" style="width:85%; margin-right:0px; margin-top:10px;margin-bottom:0px;">
          </div>

        </br>

          <p class="paragraph-3 nerf_text", style="text-align:center">
            Figure 2. (a) Word cloud of text information. (b) Examples of multi-modal data representation in UniG3D.
          </p>
     



        <div class="w-slider-arrow-left"><div class="w-icon-slider-left"></div></div>
                <div class="w-slider-arrow-right"><div class="w-icon-slider-right"></div></div>
                <div class="nerf_slide_nav w-slider-nav w-slider-nav-invert w-round"></div></div>
        </div>
    </div>

    <div class="section">
        <s2>Showcase</s2>
        <hr>
        
        <div class="columns-5 w-row">
                <img src="imgs/word_cloud.png" style="width:90%; margin-right:0px; margin-top:10px;margin-bottom:0px;">
          </div>

        </br>

          <p class="paragraph-3 nerf_text", style="text-align:center">
            Figure 2. (a) Word cloud of text information. (b) Examples of multi-modal data representation in UniG3D.
          </p>
     



        <div class="w-slider-arrow-left"><div class="w-icon-slider-left"></div></div>
                <div class="w-slider-arrow-right"><div class="w-icon-slider-right"></div></div>
                <div class="nerf_slide_nav w-slider-nav w-slider-nav-invert w-round"></div></div>
        </div>
    </div>





<!--         </br></br></br>
        <s2> Concurrent works </s2>
        <hr>
        <p class="paragraph-3 nerf_text">
        Some concurrent works also focus on building large-scale 3D object datasets:</p>
         <ul>
            <li class="paragraph-3 nerf_text", style="text-align: left; color:#585858"><a href="https://objaverse.allenai.org/" target="_blank"> Objaverse</a> is a massive dataset with 800K+ annotated 3D objects collected from <a href="https://sketchfab.com/feed" target="_blank"> Sketchfab</a>.
            </li>
            <li class="paragraph-3 nerf_text", style="text-align: left; color:#585858"><a href="https://eyecan-ai.github.io/scannerf/" target="_blank">ScanNeRF</a> provides an effective pipeline for scanning real objects in quantity and effortlessly for evaluating Neural Rendering frameworks.</li>
         </ul> -->
    <!-- </div> -->
        



        <!-- </br></br></br>
        <s2> Acknowledgements </s2>
        <hr>
        <p class="paragraph-3 nerf_text">
            We would like to thank  Yannick Hold-Geoffroy for his useful suggestion in video animation,
            Qiangeng Xu for providing some baseline results,
            and, Katja Schwarz and Michael Niemeyer for providing helpful video materials.
            This project was supported by NSF grant IIS-1764078 and gift money from VIVO.
        </p> -->

<!--    <div class="section">-->
<!--         <s2>Bibtex</s2>
        <hr>
        <div class="bibtexsection">
            @article{wu2023omniobject3d,
              author = {Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, 
              Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, Dahua Lin, Ziwei Liu},
              title = {OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, 
                Reconstruction and Generation},
                journal={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
                year={2023}
            }
        </div> -->
<!--    </div>-->
<!--     </div>
    <hr>

</div> -->


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd6c33218.js" type="text/javascript"></script>

<!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->

</body>
</html>
