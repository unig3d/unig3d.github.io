# Background

Due to the large storage space required by the entire dataset, we have not found a suitable way to release the quadruples data other than text to the public. However, we provide a detailed data transformation pipeline. Based on this pipeline, users can easily obtain data consistent with our dataset.

Please configure the Blender environment on your machine first.

# 0-1.Mesh

Below is a script that can be run within Blender to render a 3D model as an 'obj' format mesh. Go to `0_mesh-obj` folder. Example usage: `blender -b -P glb2obj.py input_path`

Then, use the following script to convert the obj format to sdf format. Copy `isosurface` folder from https://github.com/yccyenchicheng/SDFusion/tree/master/preprocess/isosurface to `1_mesh-sdf` folder.  Example usage: `python obj2sdf.py --model obj_path --reduce 4 --save_path sdf_path`

`obj_path` is the path to the input obj formated mesh. `reduce`  refers to the downsampling ratio, where the mesh is first generated at a resolution of 256, and then downsampled by a factor of 4 to obtain a mesh with a resolution of 64. `save_path` is the path to the output sdf formated mesh.


# 2.Image

Script to run within Blender to render a 3D model as RGBAD images. Go to `2_img` folder. Example usage:  `blender -b -P glb2rgbd.py input_path output_dir`

Pass `--camera_pose z-circular-elevated` for the rendering used to rendering the images with z-circular camera pose. Pass `--camera_pose random` for the rendering used to rendering the images with random camera pose. 

The output directory will include metadata json files for each rendered view, as well as a global metadata file for the render. Each image will be saved as a collection of 16-bit PNG files for each channel (RGBAD), as well as a full grayscale render of the view.

If the rendered images are found to be relatively dark, please use the 'img_bright.py' script to increase the image brightness. Example usage: `python img_bright.py input_dir`. The input directory refers to the `output_dir` of previous step.

# 3.Point Cloud

To convert the 3D models into colored point clouds, we utilize the RGBAD images with random camera poses obtained in the second step.

Go to `3_pcl` folder. Example usage:  `python rgbd2pcd.py output_dir`

`output_dir` refers to the output directory of the second step.

# 4.Text

The textual information of our dataset consists of five parts: 1) the model name uploaded by the user, 2) the model description uploaded by the user, 3) the tags uploaded by the user, 4) the captions generated by the multimodal model, and 5) the category names extracted based on the captions generated in 4).

For any given data in our dataset, it may be missing one or more of the above five types of information. If any part is missing, we use `empty` as a placeholder. Since the information in the above five parts may not match the model, we provide clip scores for data cleaning or filtering."

The textual information for the UniG3D-Shapenet and UniG3D-Objaverse datasets can be found in the `4_text` folder. Specifically, each line represents the textual information for each data, with the format of:`"{data_path}\t{model-name}\t{score_model-name}\t{model-desc}\t{score_model-desc}\t{model-tags}\t{model-caption}\t{score_model-caption}\t{model-category}\t{score_model-category}\n`
